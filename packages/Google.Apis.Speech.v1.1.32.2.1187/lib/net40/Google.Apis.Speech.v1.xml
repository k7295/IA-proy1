<?xml version="1.0"?>
<doc>
    <assembly>
        <name>Google.Apis.Speech.v1</name>
    </assembly>
    <members>
        <member name="T:Google.Apis.Speech.v1.SpeechService">
            <summary>The Speech Service.</summary>
        </member>
        <member name="F:Google.Apis.Speech.v1.SpeechService.Version">
            <summary>The API version.</summary>
        </member>
        <member name="F:Google.Apis.Speech.v1.SpeechService.DiscoveryVersionUsed">
            <summary>The discovery version used to generate this service.</summary>
        </member>
        <member name="M:Google.Apis.Speech.v1.SpeechService.#ctor">
            <summary>Constructs a new service.</summary>
        </member>
        <member name="M:Google.Apis.Speech.v1.SpeechService.#ctor(Google.Apis.Services.BaseClientService.Initializer)">
            <summary>Constructs a new service.</summary>
            <param name="initializer">The service initializer.</param>
        </member>
        <member name="P:Google.Apis.Speech.v1.SpeechService.Features">
            <summary>Gets the service supported features.</summary>
        </member>
        <member name="P:Google.Apis.Speech.v1.SpeechService.Name">
            <summary>Gets the service name.</summary>
        </member>
        <member name="P:Google.Apis.Speech.v1.SpeechService.BaseUri">
            <summary>Gets the service base URI.</summary>
        </member>
        <member name="P:Google.Apis.Speech.v1.SpeechService.BasePath">
            <summary>Gets the service base path.</summary>
        </member>
        <member name="T:Google.Apis.Speech.v1.SpeechService.Scope">
            <summary>Available OAuth 2.0 scopes for use with the Cloud Speech API.</summary>
        </member>
        <member name="F:Google.Apis.Speech.v1.SpeechService.Scope.CloudPlatform">
            <summary>View and manage your data across Google Cloud Platform services</summary>
        </member>
        <member name="P:Google.Apis.Speech.v1.SpeechService.Operations">
            <summary>Gets the Operations resource.</summary>
        </member>
        <member name="P:Google.Apis.Speech.v1.SpeechService.Speech">
            <summary>Gets the Speech resource.</summary>
        </member>
        <member name="T:Google.Apis.Speech.v1.SpeechBaseServiceRequest`1">
            <summary>A base abstract class for Speech requests.</summary>
        </member>
        <member name="M:Google.Apis.Speech.v1.SpeechBaseServiceRequest`1.#ctor(Google.Apis.Services.IClientService)">
            <summary>Constructs a new SpeechBaseServiceRequest instance.</summary>
        </member>
        <member name="P:Google.Apis.Speech.v1.SpeechBaseServiceRequest`1.Xgafv">
            <summary>V1 error format.</summary>
        </member>
        <member name="T:Google.Apis.Speech.v1.SpeechBaseServiceRequest`1.XgafvEnum">
            <summary>V1 error format.</summary>
        </member>
        <member name="F:Google.Apis.Speech.v1.SpeechBaseServiceRequest`1.XgafvEnum.Value1">
            <summary>v1 error format</summary>
        </member>
        <member name="F:Google.Apis.Speech.v1.SpeechBaseServiceRequest`1.XgafvEnum.Value2">
            <summary>v2 error format</summary>
        </member>
        <member name="P:Google.Apis.Speech.v1.SpeechBaseServiceRequest`1.AccessToken">
            <summary>OAuth access token.</summary>
        </member>
        <member name="P:Google.Apis.Speech.v1.SpeechBaseServiceRequest`1.Alt">
            <summary>Data format for response.</summary>
            [default: json]
        </member>
        <member name="T:Google.Apis.Speech.v1.SpeechBaseServiceRequest`1.AltEnum">
            <summary>Data format for response.</summary>
        </member>
        <member name="F:Google.Apis.Speech.v1.SpeechBaseServiceRequest`1.AltEnum.Json">
            <summary>Responses with Content-Type of application/json</summary>
        </member>
        <member name="F:Google.Apis.Speech.v1.SpeechBaseServiceRequest`1.AltEnum.Media">
            <summary>Media download with context-dependent Content-Type</summary>
        </member>
        <member name="F:Google.Apis.Speech.v1.SpeechBaseServiceRequest`1.AltEnum.Proto">
            <summary>Responses with Content-Type of application/x-protobuf</summary>
        </member>
        <member name="P:Google.Apis.Speech.v1.SpeechBaseServiceRequest`1.BearerToken">
            <summary>OAuth bearer token.</summary>
        </member>
        <member name="P:Google.Apis.Speech.v1.SpeechBaseServiceRequest`1.Callback">
            <summary>JSONP</summary>
        </member>
        <member name="P:Google.Apis.Speech.v1.SpeechBaseServiceRequest`1.Fields">
            <summary>Selector specifying which fields to include in a partial response.</summary>
        </member>
        <member name="P:Google.Apis.Speech.v1.SpeechBaseServiceRequest`1.Key">
            <summary>API key. Your API key identifies your project and provides you with API access, quota, and reports.
            Required unless you provide an OAuth 2.0 token.</summary>
        </member>
        <member name="P:Google.Apis.Speech.v1.SpeechBaseServiceRequest`1.OauthToken">
            <summary>OAuth 2.0 token for the current user.</summary>
        </member>
        <member name="P:Google.Apis.Speech.v1.SpeechBaseServiceRequest`1.Pp">
            <summary>Pretty-print response.</summary>
            [default: true]
        </member>
        <member name="P:Google.Apis.Speech.v1.SpeechBaseServiceRequest`1.PrettyPrint">
            <summary>Returns response with indentations and line breaks.</summary>
            [default: true]
        </member>
        <member name="P:Google.Apis.Speech.v1.SpeechBaseServiceRequest`1.QuotaUser">
            <summary>Available to use for quota purposes for server-side applications. Can be any arbitrary string
            assigned to a user, but should not exceed 40 characters.</summary>
        </member>
        <member name="P:Google.Apis.Speech.v1.SpeechBaseServiceRequest`1.UploadType">
            <summary>Legacy upload protocol for media (e.g. "media", "multipart").</summary>
        </member>
        <member name="P:Google.Apis.Speech.v1.SpeechBaseServiceRequest`1.UploadProtocol">
            <summary>Upload protocol for media (e.g. "raw", "multipart").</summary>
        </member>
        <member name="M:Google.Apis.Speech.v1.SpeechBaseServiceRequest`1.InitParameters">
            <summary>Initializes Speech parameter list.</summary>
        </member>
        <member name="T:Google.Apis.Speech.v1.OperationsResource">
            <summary>The "operations" collection of methods.</summary>
        </member>
        <member name="F:Google.Apis.Speech.v1.OperationsResource.service">
            <summary>The service which this resource belongs to.</summary>
        </member>
        <member name="M:Google.Apis.Speech.v1.OperationsResource.#ctor(Google.Apis.Services.IClientService)">
            <summary>Constructs a new resource.</summary>
        </member>
        <member name="M:Google.Apis.Speech.v1.OperationsResource.Get(System.String)">
            <summary>Gets the latest state of a long-running operation.  Clients can use this method to poll the
            operation result at intervals as recommended by the API service.</summary>
            <param name="name">The name of the operation resource.</param>
        </member>
        <member name="T:Google.Apis.Speech.v1.OperationsResource.GetRequest">
            <summary>Gets the latest state of a long-running operation.  Clients can use this method to poll the
            operation result at intervals as recommended by the API service.</summary>
        </member>
        <member name="M:Google.Apis.Speech.v1.OperationsResource.GetRequest.#ctor(Google.Apis.Services.IClientService,System.String)">
            <summary>Constructs a new Get request.</summary>
        </member>
        <member name="P:Google.Apis.Speech.v1.OperationsResource.GetRequest.Name">
            <summary>The name of the operation resource.</summary>
        </member>
        <member name="P:Google.Apis.Speech.v1.OperationsResource.GetRequest.MethodName">
            <summary>Gets the method name.</summary>
        </member>
        <member name="P:Google.Apis.Speech.v1.OperationsResource.GetRequest.HttpMethod">
            <summary>Gets the HTTP method.</summary>
        </member>
        <member name="P:Google.Apis.Speech.v1.OperationsResource.GetRequest.RestPath">
            <summary>Gets the REST path.</summary>
        </member>
        <member name="M:Google.Apis.Speech.v1.OperationsResource.GetRequest.InitParameters">
            <summary>Initializes Get parameter list.</summary>
        </member>
        <member name="T:Google.Apis.Speech.v1.SpeechResource">
            <summary>The "speech" collection of methods.</summary>
        </member>
        <member name="F:Google.Apis.Speech.v1.SpeechResource.service">
            <summary>The service which this resource belongs to.</summary>
        </member>
        <member name="M:Google.Apis.Speech.v1.SpeechResource.#ctor(Google.Apis.Services.IClientService)">
            <summary>Constructs a new resource.</summary>
        </member>
        <member name="M:Google.Apis.Speech.v1.SpeechResource.Longrunningrecognize(Google.Apis.Speech.v1.Data.LongRunningRecognizeRequest)">
            <summary>Performs asynchronous speech recognition: receive results via the google.longrunning.Operations
            interface. Returns either an `Operation.error` or an `Operation.response` which contains a
            `LongRunningRecognizeResponse` message.</summary>
            <param name="body">The body of the request.</param>
        </member>
        <member name="T:Google.Apis.Speech.v1.SpeechResource.LongrunningrecognizeRequest">
            <summary>Performs asynchronous speech recognition: receive results via the google.longrunning.Operations
            interface. Returns either an `Operation.error` or an `Operation.response` which contains a
            `LongRunningRecognizeResponse` message.</summary>
        </member>
        <member name="M:Google.Apis.Speech.v1.SpeechResource.LongrunningrecognizeRequest.#ctor(Google.Apis.Services.IClientService,Google.Apis.Speech.v1.Data.LongRunningRecognizeRequest)">
            <summary>Constructs a new Longrunningrecognize request.</summary>
        </member>
        <member name="P:Google.Apis.Speech.v1.SpeechResource.LongrunningrecognizeRequest.Body">
            <summary>Gets or sets the body of this request.</summary>
        </member>
        <member name="M:Google.Apis.Speech.v1.SpeechResource.LongrunningrecognizeRequest.GetBody">
            <summary>Returns the body of the request.</summary>
        </member>
        <member name="P:Google.Apis.Speech.v1.SpeechResource.LongrunningrecognizeRequest.MethodName">
            <summary>Gets the method name.</summary>
        </member>
        <member name="P:Google.Apis.Speech.v1.SpeechResource.LongrunningrecognizeRequest.HttpMethod">
            <summary>Gets the HTTP method.</summary>
        </member>
        <member name="P:Google.Apis.Speech.v1.SpeechResource.LongrunningrecognizeRequest.RestPath">
            <summary>Gets the REST path.</summary>
        </member>
        <member name="M:Google.Apis.Speech.v1.SpeechResource.LongrunningrecognizeRequest.InitParameters">
            <summary>Initializes Longrunningrecognize parameter list.</summary>
        </member>
        <member name="M:Google.Apis.Speech.v1.SpeechResource.Recognize(Google.Apis.Speech.v1.Data.RecognizeRequest)">
            <summary>Performs synchronous speech recognition: receive results after all audio has been sent and
            processed.</summary>
            <param name="body">The body of the request.</param>
        </member>
        <member name="T:Google.Apis.Speech.v1.SpeechResource.RecognizeRequest">
            <summary>Performs synchronous speech recognition: receive results after all audio has been sent and
            processed.</summary>
        </member>
        <member name="M:Google.Apis.Speech.v1.SpeechResource.RecognizeRequest.#ctor(Google.Apis.Services.IClientService,Google.Apis.Speech.v1.Data.RecognizeRequest)">
            <summary>Constructs a new Recognize request.</summary>
        </member>
        <member name="P:Google.Apis.Speech.v1.SpeechResource.RecognizeRequest.Body">
            <summary>Gets or sets the body of this request.</summary>
        </member>
        <member name="M:Google.Apis.Speech.v1.SpeechResource.RecognizeRequest.GetBody">
            <summary>Returns the body of the request.</summary>
        </member>
        <member name="P:Google.Apis.Speech.v1.SpeechResource.RecognizeRequest.MethodName">
            <summary>Gets the method name.</summary>
        </member>
        <member name="P:Google.Apis.Speech.v1.SpeechResource.RecognizeRequest.HttpMethod">
            <summary>Gets the HTTP method.</summary>
        </member>
        <member name="P:Google.Apis.Speech.v1.SpeechResource.RecognizeRequest.RestPath">
            <summary>Gets the REST path.</summary>
        </member>
        <member name="M:Google.Apis.Speech.v1.SpeechResource.RecognizeRequest.InitParameters">
            <summary>Initializes Recognize parameter list.</summary>
        </member>
        <member name="T:Google.Apis.Speech.v1.Data.LongRunningRecognizeRequest">
            <summary>The top-level message sent by the client for the `LongRunningRecognize` method.</summary>
        </member>
        <member name="P:Google.Apis.Speech.v1.Data.LongRunningRecognizeRequest.Audio">
            <summary>*Required* The audio data to be recognized.</summary>
        </member>
        <member name="P:Google.Apis.Speech.v1.Data.LongRunningRecognizeRequest.Config">
            <summary>*Required* Provides information to the recognizer that specifies how to process the
            request.</summary>
        </member>
        <member name="P:Google.Apis.Speech.v1.Data.LongRunningRecognizeRequest.ETag">
            <summary>The ETag of the item.</summary>
        </member>
        <member name="T:Google.Apis.Speech.v1.Data.Operation">
            <summary>This resource represents a long-running operation that is the result of a network API call.</summary>
        </member>
        <member name="P:Google.Apis.Speech.v1.Data.Operation.Done">
            <summary>If the value is `false`, it means the operation is still in progress. If `true`, the operation is
            completed, and either `error` or `response` is available.</summary>
        </member>
        <member name="P:Google.Apis.Speech.v1.Data.Operation.Error">
            <summary>The error result of the operation in case of failure or cancellation.</summary>
        </member>
        <member name="P:Google.Apis.Speech.v1.Data.Operation.Metadata">
            <summary>Service-specific metadata associated with the operation.  It typically contains progress
            information and common metadata such as create time. Some services might not provide such metadata.  Any
            method that returns a long-running operation should document the metadata type, if any.</summary>
        </member>
        <member name="P:Google.Apis.Speech.v1.Data.Operation.Name">
            <summary>The server-assigned name, which is only unique within the same service that originally returns it.
            If you use the default HTTP mapping, the `name` should have the format of
            `operations/some/unique/name`.</summary>
        </member>
        <member name="P:Google.Apis.Speech.v1.Data.Operation.Response">
            <summary>The normal response of the operation in case of success.  If the original method returns no data on
            success, such as `Delete`, the response is `google.protobuf.Empty`.  If the original method is standard
            `Get`/`Create`/`Update`, the response should be the resource.  For other methods, the response should have
            the type `XxxResponse`, where `Xxx` is the original method name.  For example, if the original method name
            is `TakeSnapshot()`, the inferred response type is `TakeSnapshotResponse`.</summary>
        </member>
        <member name="P:Google.Apis.Speech.v1.Data.Operation.ETag">
            <summary>The ETag of the item.</summary>
        </member>
        <member name="T:Google.Apis.Speech.v1.Data.RecognitionAudio">
            <summary>Contains audio data in the encoding specified in the `RecognitionConfig`. Either `content` or `uri`
            must be supplied. Supplying both or neither returns google.rpc.Code.INVALID_ARGUMENT. See [audio
            limits](https://cloud.google.com/speech/limits#content).</summary>
        </member>
        <member name="P:Google.Apis.Speech.v1.Data.RecognitionAudio.Content">
            <summary>The audio data bytes encoded as specified in `RecognitionConfig`. Note: as with all bytes fields,
            protobuffers use a pure binary representation, whereas JSON representations use base64.</summary>
        </member>
        <member name="P:Google.Apis.Speech.v1.Data.RecognitionAudio.Uri">
            <summary>URI that points to a file that contains audio data bytes as specified in `RecognitionConfig`.
            Currently, only Google Cloud Storage URIs are supported, which must be specified in the following format:
            `gs://bucket_name/object_name` (other URI formats return google.rpc.Code.INVALID_ARGUMENT). For more
            information, see [Request URIs](https://cloud.google.com/storage/docs/reference-uris).</summary>
        </member>
        <member name="P:Google.Apis.Speech.v1.Data.RecognitionAudio.ETag">
            <summary>The ETag of the item.</summary>
        </member>
        <member name="T:Google.Apis.Speech.v1.Data.RecognitionConfig">
            <summary>Provides information to the recognizer that specifies how to process the request.</summary>
        </member>
        <member name="P:Google.Apis.Speech.v1.Data.RecognitionConfig.EnableWordTimeOffsets">
            <summary>*Optional* If `true`, the top result includes a list of words and the start and end time offsets
            (timestamps) for those words. If `false`, no word-level time offset information is returned. The default is
            `false`.</summary>
        </member>
        <member name="P:Google.Apis.Speech.v1.Data.RecognitionConfig.Encoding">
            <summary>Encoding of audio data sent in all `RecognitionAudio` messages. This field is optional for `FLAC`
            and `WAV` audio files and required for all other audio formats. For details, see AudioEncoding.</summary>
        </member>
        <member name="P:Google.Apis.Speech.v1.Data.RecognitionConfig.LanguageCode">
            <summary>*Required* The language of the supplied audio as a [BCP-47](https://www.rfc-
            editor.org/rfc/bcp/bcp47.txt) language tag. Example: "en-US". See [Language
            Support](https://cloud.google.com/speech/docs/languages) for a list of the currently supported language
            codes.</summary>
        </member>
        <member name="P:Google.Apis.Speech.v1.Data.RecognitionConfig.MaxAlternatives">
            <summary>*Optional* Maximum number of recognition hypotheses to be returned. Specifically, the maximum
            number of `SpeechRecognitionAlternative` messages within each `SpeechRecognitionResult`. The server may
            return fewer than `max_alternatives`. Valid values are `0`-`30`. A value of `0` or `1` will return a maximum
            of one. If omitted, will return a maximum of one.</summary>
        </member>
        <member name="P:Google.Apis.Speech.v1.Data.RecognitionConfig.ProfanityFilter">
            <summary>*Optional* If set to `true`, the server will attempt to filter out profanities, replacing all but
            the initial character in each filtered word with asterisks, e.g. "f***". If set to `false` or omitted,
            profanities won't be filtered out.</summary>
        </member>
        <member name="P:Google.Apis.Speech.v1.Data.RecognitionConfig.SampleRateHertz">
            <summary>Sample rate in Hertz of the audio data sent in all `RecognitionAudio` messages. Valid values are:
            8000-48000. 16000 is optimal. For best results, set the sampling rate of the audio source to 16000 Hz. If
            that's not possible, use the native sample rate of the audio source (instead of re-sampling). This field is
            optional for `FLAC` and `WAV` audio files and required for all other audio formats. For details, see
            AudioEncoding.</summary>
        </member>
        <member name="P:Google.Apis.Speech.v1.Data.RecognitionConfig.SpeechContexts">
            <summary>*Optional* A means to provide context to assist the speech recognition.</summary>
        </member>
        <member name="P:Google.Apis.Speech.v1.Data.RecognitionConfig.ETag">
            <summary>The ETag of the item.</summary>
        </member>
        <member name="T:Google.Apis.Speech.v1.Data.RecognizeRequest">
            <summary>The top-level message sent by the client for the `Recognize` method.</summary>
        </member>
        <member name="P:Google.Apis.Speech.v1.Data.RecognizeRequest.Audio">
            <summary>*Required* The audio data to be recognized.</summary>
        </member>
        <member name="P:Google.Apis.Speech.v1.Data.RecognizeRequest.Config">
            <summary>*Required* Provides information to the recognizer that specifies how to process the
            request.</summary>
        </member>
        <member name="P:Google.Apis.Speech.v1.Data.RecognizeRequest.ETag">
            <summary>The ETag of the item.</summary>
        </member>
        <member name="T:Google.Apis.Speech.v1.Data.RecognizeResponse">
            <summary>The only message returned to the client by the `Recognize` method. It contains the result as zero or
            more sequential `SpeechRecognitionResult` messages.</summary>
        </member>
        <member name="P:Google.Apis.Speech.v1.Data.RecognizeResponse.Results">
            <summary>Output only. Sequential list of transcription results corresponding to sequential portions of
            audio.</summary>
        </member>
        <member name="P:Google.Apis.Speech.v1.Data.RecognizeResponse.ETag">
            <summary>The ETag of the item.</summary>
        </member>
        <member name="T:Google.Apis.Speech.v1.Data.SpeechContext">
            <summary>Provides "hints" to the speech recognizer to favor specific words and phrases in the results.</summary>
        </member>
        <member name="P:Google.Apis.Speech.v1.Data.SpeechContext.Phrases">
            <summary>*Optional* A list of strings containing words and phrases "hints" so that the speech recognition is
            more likely to recognize them. This can be used to improve the accuracy for specific words and phrases, for
            example, if specific commands are typically spoken by the user. This can also be used to add additional
            words to the vocabulary of the recognizer. See [usage
            limits](https://cloud.google.com/speech/limits#content).</summary>
        </member>
        <member name="P:Google.Apis.Speech.v1.Data.SpeechContext.ETag">
            <summary>The ETag of the item.</summary>
        </member>
        <member name="T:Google.Apis.Speech.v1.Data.SpeechRecognitionAlternative">
            <summary>Alternative hypotheses (a.k.a. n-best list).</summary>
        </member>
        <member name="P:Google.Apis.Speech.v1.Data.SpeechRecognitionAlternative.Confidence">
            <summary>Output only. The confidence estimate between 0.0 and 1.0. A higher number indicates an estimated
            greater likelihood that the recognized words are correct. This field is set only for the top alternative of
            a non-streaming result or, of a streaming result where `is_final=true`. This field is not guaranteed to be
            accurate and users should not rely on it to be always provided. The default of 0.0 is a sentinel value
            indicating `confidence` was not set.</summary>
        </member>
        <member name="P:Google.Apis.Speech.v1.Data.SpeechRecognitionAlternative.Transcript">
            <summary>Output only. Transcript text representing the words that the user spoke.</summary>
        </member>
        <member name="P:Google.Apis.Speech.v1.Data.SpeechRecognitionAlternative.Words">
            <summary>Output only. A list of word-specific information for each recognized word.</summary>
        </member>
        <member name="P:Google.Apis.Speech.v1.Data.SpeechRecognitionAlternative.ETag">
            <summary>The ETag of the item.</summary>
        </member>
        <member name="T:Google.Apis.Speech.v1.Data.SpeechRecognitionResult">
            <summary>A speech recognition result corresponding to a portion of the audio.</summary>
        </member>
        <member name="P:Google.Apis.Speech.v1.Data.SpeechRecognitionResult.Alternatives">
            <summary>Output only. May contain one or more recognition hypotheses (up to the maximum specified in
            `max_alternatives`). These alternatives are ordered in terms of accuracy, with the top (first) alternative
            being the most probable, as ranked by the recognizer.</summary>
        </member>
        <member name="P:Google.Apis.Speech.v1.Data.SpeechRecognitionResult.ETag">
            <summary>The ETag of the item.</summary>
        </member>
        <member name="T:Google.Apis.Speech.v1.Data.Status">
             <summary>The `Status` type defines a logical error model that is suitable for different programming
             environments, including REST APIs and RPC APIs. It is used by [gRPC](https://github.com/grpc). The error model
             is designed to be:
            
             - Simple to use and understand for most users - Flexible enough to meet unexpected needs
            
             # Overview
            
             The `Status` message contains three pieces of data: error code, error message, and error details. The error code
             should be an enum value of google.rpc.Code, but it may accept additional error codes if needed.  The error
             message should be a developer-facing English message that helps developers *understand* and *resolve* the error.
             If a localized user-facing error message is needed, put the localized message in the error details or localize
             it in the client. The optional error details may contain arbitrary information about the error. There is a
             predefined set of error detail types in the package `google.rpc` that can be used for common error conditions.
            
             # Language mapping
            
             The `Status` message is the logical representation of the error model, but it is not necessarily the actual wire
             format. When the `Status` message is exposed in different client libraries and different wire protocols, it can
             be mapped differently. For example, it will likely be mapped to some exceptions in Java, but more likely mapped
             to some error codes in C.
            
             # Other uses
            
             The error model and the `Status` message can be used in a variety of environments, either with or without APIs,
             to provide a consistent developer experience across different environments.
            
             Example uses of this error model include:
            
             - Partial errors. If a service needs to return partial errors to the client, it may embed the `Status` in the
             normal response to indicate the partial errors.
            
             - Workflow errors. A typical workflow has multiple steps. Each step may have a `Status` message for error
             reporting.
            
             - Batch operations. If a client uses batch request and batch response, the `Status` message should be used
             directly inside batch response, one for each error sub-response.
            
             - Asynchronous operations. If an API call embeds asynchronous operation results in its response, the status of
             those operations should be represented directly using the `Status` message.
            
             - Logging. If some API errors are stored in logs, the message `Status` could be used directly after any
             stripping needed for security/privacy reasons.</summary>
        </member>
        <member name="P:Google.Apis.Speech.v1.Data.Status.Code">
            <summary>The status code, which should be an enum value of google.rpc.Code.</summary>
        </member>
        <member name="P:Google.Apis.Speech.v1.Data.Status.Details">
            <summary>A list of messages that carry the error details.  There is a common set of message types for APIs
            to use.</summary>
        </member>
        <member name="P:Google.Apis.Speech.v1.Data.Status.Message">
            <summary>A developer-facing error message, which should be in English. Any user-facing error message should
            be localized and sent in the google.rpc.Status.details field, or localized by the client.</summary>
        </member>
        <member name="P:Google.Apis.Speech.v1.Data.Status.ETag">
            <summary>The ETag of the item.</summary>
        </member>
        <member name="T:Google.Apis.Speech.v1.Data.WordInfo">
            <summary>Word-specific information for recognized words.</summary>
        </member>
        <member name="P:Google.Apis.Speech.v1.Data.WordInfo.EndTime">
            <summary>Output only. Time offset relative to the beginning of the audio, and corresponding to the end of
            the spoken word. This field is only set if `enable_word_time_offsets=true` and only in the top hypothesis.
            This is an experimental feature and the accuracy of the time offset can vary.</summary>
        </member>
        <member name="P:Google.Apis.Speech.v1.Data.WordInfo.StartTime">
            <summary>Output only. Time offset relative to the beginning of the audio, and corresponding to the start of
            the spoken word. This field is only set if `enable_word_time_offsets=true` and only in the top hypothesis.
            This is an experimental feature and the accuracy of the time offset can vary.</summary>
        </member>
        <member name="P:Google.Apis.Speech.v1.Data.WordInfo.Word">
            <summary>Output only. The word corresponding to this set of information.</summary>
        </member>
        <member name="P:Google.Apis.Speech.v1.Data.WordInfo.ETag">
            <summary>The ETag of the item.</summary>
        </member>
    </members>
</doc>
